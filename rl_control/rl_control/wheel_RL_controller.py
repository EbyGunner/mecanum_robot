import rclpy
from rclpy.node import Node
from std_msgs.msg import Float64MultiArray
from sensor_msgs.msg import LaserScan
from message_interfaces.msg import GoalCurrentPose
import os, sys
import numpy as np
import math
import time
from datetime import datetime

SRC_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'src'))
EXT_LIB_PATH = os.path.join(SRC_DIR, 'mecanum_robot', 'rl_control', 'external_libraries')
sys.path.insert(0, EXT_LIB_PATH)

import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.callbacks import EvalCallback
import torch.nn as nn

class MecanumRLController(Node, gym.Env):
    def __init__(self):
        # Initialize both parent classes
        super().__init__('mecanum_rl_controller')
        gym.Env.__init__(self)
        
        # Mecanum wheel configuration
        self.wheel_vel_pub = self.create_publisher(Float64MultiArray, '/wheel_velocity_controller/commands', 10)
        
        # Optimized observation space with reduced lidar dimensions
        self.observation_space = gym.spaces.Dict({
            "lidar": gym.spaces.Box(low=0, high=30, shape=(360,), dtype=np.float32),
            "position": gym.spaces.Box(low=-10, high=10, shape=(2,), dtype=np.float32),
            "orientation": gym.spaces.Box(low=-np.pi, high=np.pi, shape=(1,), dtype=np.float32),
            "goal_position": gym.spaces.Box(low=-10, high=10, shape=(2,), dtype=np.float32),
            "goal_distance": gym.spaces.Box(low=0, high=20, shape=(1,), dtype=np.float32),
            "goal_angle": gym.spaces.Box(low=-np.pi, high=np.pi, shape=(1,), dtype=np.float32)
        })

        # Action space: individual wheel velocities (rad/s)
        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(4,), dtype=np.float32)

        # Robot state variables
        self.current_position = np.zeros(2)
        self.current_orientation = np.zeros(1)
        self.goal_position = np.zeros(2)
        self.lidar_data = np.zeros(360)
        
        # Subscribers
        self.lidar_sub = self.create_subscription(LaserScan, '/scan', self.lidar_callback, 10)
        self.goal_pos_sub = self.create_subscription(GoalCurrentPose, '/goal_current_pose', self.goal_cpos_callback, 10)

        self.get_logger().info(f"Subscribed to lidar topic: {self.lidar_sub.topic_name}")
        self.get_logger().info(f"Subscribed to goal topic: {self.goal_pos_sub.topic_name}")

        self.goal_position = None
        self.previous_distance = None

        # RL parameters
        self.episode_length = 5000
        self.current_step = 0
        self.episode_reward = 0
        self.done = False
        self.success_threshold = 0.05

        # Progress monitoring - optimized
        self.position_buffer = np.zeros((10, 2))
        self.buffer_index = 0
        self.min_position_change = 0.02
        
        # Speed control parameters
        self.normal_speed = 20.0
        self.boosted_speed = 30.0
        self.current_max_speed = self.normal_speed

        # Enhanced action scaling
        self.max_initial_speed = 30.0
        self.min_speed = 5.0
        self.distance_threshold = 1.0

        # Recovery system - simplified and optimized
        self.off_course_threshold = math.pi/8
        self.recovery_duration = 0
        self.max_recovery_steps = 15
        self.recovery_cooldown = 0
        self.collision_threshold = 0.35
        self.last_recovery_type = "none"
        
        # FIXED: Proper collision detection zones using tuple of slices
        self.collision_zones = {
            'front': (slice(0, 45), slice(-45, None)),  # Front 90 degrees
            'front_sides': (slice(45, 90), slice(-90, -45)),
        }
        
        # State tracking
        self.last_actions = np.zeros((5, 4))
        self.action_index = 0
        self.oscillation_counter = 0
        self.max_oscillations = 3
        
        # Pre-computed constants
        self.control_period = 0.1
        
        # Model saving parameters
        self.successful_episodes = 0
        self.models_saved = 0
        self.max_models_to_save = 20  # Increased limit for date-time folders
        self.last_success_time = 0
        self.success_cooldown = 5  # Minimum seconds between model saves
        
        # Training session management
        self.current_session_dir = None
        self.session_start_time = None
        
        # Initialize RL model with optimized architecture
        self.model = self._init_model()
        
        # Control timer (10Hz)
        self.control_timer = self.create_timer(self.control_period, self.control_loop)

        self.get_logger().info("Optimized Mecanum RL Controller initialized")

    def _init_model(self):
        """Optimized network architecture"""
        policy_kwargs = dict(
            activation_fn=nn.ReLU,
            net_arch=dict(pi=[128, 64], vf=[128, 64]),
            ortho_init=True,
            log_std_init=-0.7
        )
        
        return PPO(
            "MultiInputPolicy",
            self,
            policy_kwargs=policy_kwargs,
            verbose=1,
            learning_rate=1e-4,
            n_steps=512,
            batch_size=32,
            n_epochs=4,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            max_grad_norm=0.8,
            ent_coef=0.01,
            target_kl=0.03,
            tensorboard_log=None  # Will be set in train method
        )
    
    def _create_session_directory(self):
        """Create a new training session directory with timestamp"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_dir = os.path.join(SRC_DIR, 'mecanum_robot', 'rl_control', 'train', timestamp)
        
        # Create main session directory
        os.makedirs(session_dir, exist_ok=True)
        
        # Create subdirectories
        success_models_dir = os.path.join(session_dir, 'success_models')
        model_dir = os.path.join(session_dir, 'model')
        logs_dir = os.path.join(session_dir, 'logs')
        
        os.makedirs(success_models_dir, exist_ok=True)
        os.makedirs(model_dir, exist_ok=True)
        os.makedirs(logs_dir, exist_ok=True)
        
        self.current_session_dir = session_dir
        self.session_start_time = datetime.now()
        
        self.get_logger().info(f"ðŸ“ Created new training session: {timestamp}")
        self.get_logger().info(f"ðŸ“ Session directory: {session_dir}")
        
        return session_dir
    
    def _get_session_path(self, subpath=""):
        """Get path within current session directory"""
        if self.current_session_dir is None:
            # If no session directory exists, create one
            self._create_session_directory()
        return os.path.join(self.current_session_dir, subpath)
    
    def goal_cpos_callback(self, msg):
        """Optimized goal and pose update"""
        new_goal_x = msg.goal.pose.position.x
        new_goal_y = msg.goal.pose.position.y
        
        if (self.goal_position is None or 
            np.linalg.norm(np.array([new_goal_x, new_goal_y]) - self.goal_position) > 0.3):
            
            self.goal_position = np.array([new_goal_x, new_goal_y])
            self.get_logger().info(f"New goal: ({self.goal_position[0]:.2f}, {self.goal_position[1]:.2f})",
                                 throttle_duration_sec=2.0)

        # Update current pose
        self.current_position[0] = msg.current_transform.transform.translation.x
        self.current_position[1] = msg.current_transform.transform.translation.y
        
        # Optimized orientation calculation
        q = msg.current_transform.transform.rotation
        self.current_orientation[0] = math.atan2(2.0 * (q.w * q.z + q.x * q.y),
                                               1.0 - 2.0 * (q.y * q.y + q.z * q.z))
    
    def lidar_callback(self, msg):
        """Optimized lidar processing"""
        ranges = np.array(msg.ranges)
        
        if len(ranges) >= 360:
            step = max(1, len(ranges) // 360)
            self.lidar_data = ranges[::step][:360]
        else:
            self.lidar_data = ranges
        
        # Replace inf/nan values with max range
        self.lidar_data = np.nan_to_num(self.lidar_data, posinf=30.0, neginf=0.0)
    
    def get_obs(self):
        """Optimized observation construction"""
        if self.goal_position is None:
            self.goal_position = np.zeros(2)
        
        goal_distance = np.linalg.norm(self.current_position - self.goal_position)
        
        # Calculate goal angle relative to robot orientation
        angle_to_goal = math.atan2(self.goal_position[1] - self.current_position[1],
                                 self.goal_position[0] - self.current_position[0])
        goal_angle = (angle_to_goal - self.current_orientation[0]) % (2 * math.pi)
        if goal_angle > math.pi:
            goal_angle -= 2 * math.pi
        
        return {
            "lidar": self.lidar_data.astype(np.float32),
            "position": self.current_position.astype(np.float32),
            "orientation": self.current_orientation.astype(np.float32),
            "goal_position": self.goal_position.astype(np.float32),
            "goal_distance": np.array([goal_distance], dtype=np.float32),
            "goal_angle": np.array([goal_angle], dtype=np.float32)
        }
    
    def compute_reward(self) -> float:
        """Optimized and more stable reward function"""
        distance_to_goal = np.linalg.norm(self.current_position - self.goal_position)
        
        # Base distance reward
        distance_reward = -distance_to_goal * 0.5
        
        # Progress reward
        progress_reward = 0.0
        if self.previous_distance is not None:
            progress = self.previous_distance - distance_to_goal
            progress_reward = 2.0 * progress
        
        # Goal achievement bonus
        success_bonus = 50.0 if distance_to_goal < self.success_threshold else 0.0
        
        # Orientation reward
        angle_to_goal = math.atan2(self.goal_position[1] - self.current_position[1],
                                 self.goal_position[0] - self.current_position[0])
        angle_error = abs((angle_to_goal - self.current_orientation[0]) % (2 * math.pi))
        if angle_error > math.pi:
            angle_error = 2 * math.pi - angle_error
        
        orientation_reward = -0.3 * angle_error
        
        # Obstacle penalty - FIXED: Use proper front detection
        front_indices = list(range(0, 45)) + list(range(315, 360))
        min_lidar = np.min(self.lidar_data[front_indices])
        obstacle_penalty = -8.0 * max(0, 0.5 - min_lidar) if min_lidar < 0.5 else 0.0
        
        # Movement bonus
        movement_bonus = 0.0
        if self.buffer_index > 1:
            movement = np.linalg.norm(self.position_buffer[self.buffer_index-1] - 
                                    self.position_buffer[self.buffer_index-2])
            movement_bonus = 0.1 * movement if progress_reward > 0 else 0.0
        
        total_reward = (
            distance_reward +
            progress_reward +
            success_bonus +
            orientation_reward +
            obstacle_penalty +
            movement_bonus
        )
        
        # Update previous distance
        self.previous_distance = distance_to_goal
        
        return float(total_reward)
    
    def _save_success_model(self, distance_to_goal: float):
        """Save model after successful episode with timestamp and performance metrics"""
        current_time = time.time()
        
        # Check cooldown and model limit
        if (current_time - self.last_success_time < self.success_cooldown or 
            self.models_saved >= self.max_models_to_save):
            return
        
        try:
            # Get success models directory within current session
            success_dir = self._get_session_path("success_models")
            
            # Generate filename with timestamp and performance metrics
            timestamp = datetime.now().strftime("%H%M%S")
            filename = f"success_model_{timestamp}_ep{self.successful_episodes}_reward{self.episode_reward:.1f}_steps{self.current_step}"
            
            model_path = os.path.join(success_dir, filename)
            self.model.save(model_path)
            
            self.models_saved += 1
            self.last_success_time = current_time
            
            self.get_logger().info(
                f"ðŸŽ¯ SUCCESS! Model saved: {filename} | "
                f"Distance: {distance_to_goal:.3f}m | "
                f"Reward: {self.episode_reward:.1f} | "
                f"Steps: {self.current_step} | "
                f"Session: {os.path.basename(self.current_session_dir)}"
            )
            
        except Exception as e:
            self.get_logger().error(f"Failed to save success model: {e}")
    
    def step(self, action: np.ndarray):
        """Optimized step function with efficient recovery system"""
        # Update position buffer
        self.position_buffer[self.buffer_index] = self.current_position.copy()
        self.buffer_index = (self.buffer_index + 1) % len(self.position_buffer)
        
        # Update action history
        self.last_actions[self.action_index] = action.copy()
        self.action_index = (self.action_index + 1) % len(self.last_actions)
        
        # Check for oscillation
        if self._detect_oscillation():
            self.oscillation_counter += 1
            if self.oscillation_counter >= self.max_oscillations:
                action = self._generate_escape_pattern()
                self.oscillation_counter = 0
                self.get_logger().warn("Oscillation detected - applying escape pattern", 
                                     throttle_duration_sec=2.0)
        else:
            self.oscillation_counter = max(0, self.oscillation_counter - 0.5)
        
        # Apply recovery systems if needed
        recovery_action, recovery_type = self._check_recovery_need(action)
        if recovery_type != "none":
            blend_factor = self._get_recovery_blend_factor(recovery_type)
            action = action * (1 - blend_factor) + recovery_action * blend_factor
            self.last_recovery_type = recovery_type
        
        # Adaptive speed control
        self._update_speed_control()
        
        # Apply action scaling and publish
        real_action = np.tanh(action) * self.current_max_speed
        
        vel_msg = Float64MultiArray()
        vel_msg.data = real_action.tolist()
        self.wheel_vel_pub.publish(vel_msg)
        
        # Get observation and compute reward
        obs = self.get_obs()
        reward = self.compute_reward()
        self.episode_reward += reward
        self.current_step += 1
        
        # Check termination conditions
        self.done = self._check_termination(obs)
        
        info = {
            "distance_to_goal": obs["goal_distance"][0],
            "episode_reward": self.episode_reward,
            "recovery_active": recovery_type != "none",
            "steps": self.current_step
        }

        self.get_logger().info(
            f"Step {self.current_step}: Distance={obs['goal_distance'][0]:.3f} | "
            f"Reward={self.episode_reward:.1f} | "
            f"Recovery={recovery_type != 'none'}",
            throttle_duration_sec=0.5
        )
        
        return obs, reward, self.done, info
    
    def _detect_oscillation(self) -> bool:
        """Efficient oscillation detection"""
        if np.all(self.last_actions == 0):
            return False
        
        action_variance = np.var(self.last_actions, axis=0).mean()
        return action_variance > 0.8
    
    def _check_recovery_need(self, current_action):
        """Unified recovery need detection"""
        recovery_action = np.zeros(4)
        recovery_type = "none"
        
        # Check for collisions first
        collision_info = self._check_collision()
        if collision_info:
            recovery_action = self._get_collision_recovery(collision_info)
            recovery_type = "collision"
            self.recovery_cooldown = 10
            return recovery_action, recovery_type
        
        # Check if off course
        if (self.recovery_cooldown <= 0 and 
            self._is_off_course() and 
            self.current_step > 10):
            
            recovery_action = self._get_course_correction()
            recovery_type = "course_correction"
            self.recovery_duration = min(self.recovery_duration + 1, self.max_recovery_steps)
            return recovery_action, recovery_type
        
        # Check if stuck
        if (self.recovery_cooldown <= 0 and 
            self._is_stuck() and 
            self.current_step > 20):
            
            recovery_action = self._get_stuck_recovery()
            recovery_type = "stuck"
            self.recovery_duration = min(self.recovery_duration + 1, self.max_recovery_steps)
            return recovery_action, recovery_type
        
        # Reset recovery duration if no recovery needed
        if recovery_type == "none" and self.recovery_duration > 0:
            self.recovery_duration = max(0, self.recovery_duration - 1)
        
        # Update cooldown
        if self.recovery_cooldown > 0:
            self.recovery_cooldown -= 1
        
        return recovery_action, recovery_type
    
    def _check_collision(self):
        """Efficient collision detection"""
        # FIXED: Use proper front indices instead of slices
        front_indices = list(range(0, 45)) + list(range(315, 360))
        front_dist = np.min(self.lidar_data[front_indices])
        if front_dist < self.collision_threshold:
            return {'distance': front_dist, 'zone': 'front'}
        return None
    
    def _is_off_course(self) -> bool:
        """Check if robot is significantly off course"""
        goal_angle = self.get_obs()["goal_angle"][0]
        return abs(goal_angle) > self.off_course_threshold
    
    def _is_stuck(self) -> bool:
        """Efficient stuck detection using position buffer"""
        if self.buffer_index < 2:
            return False
        
        total_movement = 0.0
        for i in range(1, min(5, self.buffer_index)):
            idx1 = (self.buffer_index - i) % len(self.position_buffer)
            idx2 = (self.buffer_index - i - 1) % len(self.position_buffer)
            total_movement += np.linalg.norm(self.position_buffer[idx1] - self.position_buffer[idx2])
        
        return total_movement < self.min_position_change * 3
    
    def _get_collision_recovery(self, collision_info):
        """Simple collision recovery: backup and turn"""
        if collision_info['distance'] < 0.2:
            return np.array([-0.8, -0.8, -0.8, -0.8])
        else:
            return np.array([-0.3, 0.3, -0.3, 0.3])
    
    def _get_course_correction(self):
        """Efficient course correction"""
        goal_angle = self.get_obs()["goal_angle"][0]
        turn_direction = 1.0 if goal_angle > 0 else -1.0
        
        turn_strength = min(0.7, abs(goal_angle) / math.pi)
        
        return np.array([
            -turn_strength * turn_direction,
            turn_strength * turn_direction,
            -turn_strength * turn_direction,
            turn_strength * turn_direction
        ])
    
    def _get_stuck_recovery(self):
        """Simple stuck recovery pattern"""
        pattern = (self.current_step // 5) % 3
        if pattern == 0:
            return np.array([0.4, -0.4, 0.4, -0.4])
        elif pattern == 1:
            return np.array([-0.4, 0.4, -0.4, 0.4])
        else:
            return np.array([-0.3, 0.3, -0.3, 0.3])
    
    def _get_recovery_blend_factor(self, recovery_type):
        """Get blending factor for recovery actions"""
        if recovery_type == "collision":
            return 0.9
        elif recovery_type == "course_correction":
            return min(0.7, 0.3 + 0.1 * self.recovery_duration)
        else:
            return 0.6
    
    def _update_speed_control(self):
        """Efficient speed control based on situation"""
        distance_to_goal = np.linalg.norm(self.current_position - self.goal_position)
        
        if distance_to_goal > 2.0:
            self.current_max_speed = self.normal_speed
        elif distance_to_goal > 0.5:
            t = (distance_to_goal - 0.5) / 1.5
            self.current_max_speed = self.min_speed + (self.normal_speed - self.min_speed) * t
        else:
            self.current_max_speed = self.min_speed
        
        if self.last_recovery_type != "none":
            self.current_max_speed *= 0.7
    
    def _check_termination(self, obs) -> bool:
        """Efficient termination condition checking"""
        distance_to_goal = obs["goal_distance"][0]
        
        # Check for success first
        if distance_to_goal < self.success_threshold:
            self.successful_episodes += 1
            self.get_logger().info(f"ðŸŽ¯ GOAL REACHED! Distance: {distance_to_goal:.3f}m | "
                                 f"Episode: {self.successful_episodes} | "
                                 f"Reward: {self.episode_reward:.1f} | "
                                 f"Steps: {self.current_step}")
            
            # Save model on success
            self._save_success_model(distance_to_goal)
            return True
        
        if self.current_step >= self.episode_length:
            self.get_logger().info(f"Episode length exceeded. Final distance: {distance_to_goal:.2f}m")
            return True
        
        if self.recovery_duration > self.max_recovery_steps * 2:
            self.get_logger().warn("Terminating due to prolonged recovery")
            return True
        
        return False
    
    def _generate_escape_pattern(self):
        """Generate escape pattern for oscillation"""
        return np.random.uniform(-0.5, 0.5, 4)
    
    def reset(self, seed=None, options=None):
        """Optimized reset function"""
        if seed is not None:
            np.random.seed(seed)
            
        self.current_step = 0
        self.episode_reward = 0
        self.done = False
        
        self.position_buffer.fill(0)
        self.last_actions.fill(0)
        self.buffer_index = 0
        self.action_index = 0
        
        self.current_max_speed = self.normal_speed
        self.previous_distance = None
        self.recovery_duration = 0
        self.recovery_cooldown = 0
        self.last_recovery_type = "none"
        self.oscillation_counter = 0
        
        return self.get_obs(), {}
    
    def control_loop(self):
        """Optimized control loop"""
        if self.goal_position is None:
            self.get_logger().warn("Waiting for first goal position...", 
                                 throttle_duration_sec=3.0)
            return
    
        if not self.done:
            try:
                obs = self.get_obs()
                action, _ = self.model.predict(obs, deterministic=True)
                self.step(action)
            except Exception as e:
                self.get_logger().error(f"Error in control loop: {e}")
        else:
            final_distance = np.linalg.norm(self.current_position - self.goal_position)
            
            # Log episode summary
            if final_distance < self.success_threshold:
                self.get_logger().info(
                    f"ðŸŽ¯ SUCCESSFUL EPISODE: Steps={self.current_step}, "
                    f"Reward={self.episode_reward:.2f}, Distance={final_distance:.3f}m"
                )
            else:
                self.get_logger().info(
                    f"Episode ended: Steps={self.current_step}, "
                    f"Reward={self.episode_reward:.2f}, Distance={final_distance:.2f}m"
                )
            
            self.reset()
    
    def train(self, total_timesteps: int = 50000):
        """Optimized training with better logging and model saving"""
        self.get_logger().info("Starting optimized training...")
        
        # Create new session directory for this training run
        session_dir = self._create_session_directory()
        
        # Update tensorboard log to use session directory
        self.model.tensorboard_log = self._get_session_path("logs")
        
        check_env(self)

        # Custom callback to save models on success during training
        class SuccessSaveCallback(EvalCallback):
            def __init__(self, env, parent_controller, *args, **kwargs):
                super().__init__(env, *args, **kwargs)
                self.parent_controller = parent_controller
                
            def _on_step(self) -> bool:
                result = super()._on_step()
                
                # Check if current episode was successful
                if hasattr(self.parent_controller, 'done') and self.parent_controller.done:
                    distance = np.linalg.norm(
                        self.parent_controller.current_position - 
                        self.parent_controller.goal_position
                    )
                    if distance < self.parent_controller.success_threshold:
                        self.parent_controller._save_success_model(distance)
                
                return result

        eval_callback = SuccessSaveCallback(
            self,
            self,  # Pass reference to parent controller
            best_model_save_path=self._get_session_path("model"),
            log_path=self._get_session_path("logs"),
            eval_freq=2000,
            n_eval_episodes=3,
            deterministic=True,
            render=False
        )
        
        try:
            self.get_logger().info(f"Training for {total_timesteps} timesteps...")
            self.get_logger().info(f"Session directory: {session_dir}")
            
            self.model.learn(
                total_timesteps=total_timesteps, 
                callback=eval_callback,
                progress_bar=True
            )
            
            # Save final model in session directory
            final_model_path = self._get_session_path("final_model")
            self.model.save(final_model_path)
            
            # Calculate session duration
            session_duration = datetime.now() - self.session_start_time
            
            self.get_logger().info(
                f"ðŸ TRAINING COMPLETED! | "
                f"Session: {os.path.basename(session_dir)} | "
                f"Duration: {session_duration} | "
                f"Successful episodes: {self.successful_episodes} | "
                f"Models saved: {self.models_saved} | "
                f"Final model: {final_model_path}"
            )
            
        except Exception as e:
            self.get_logger().error(f"Training failed: {e}")
    
    def evaluate(self, num_episodes: int = 5):
        """Optimized evaluation"""
        self.get_logger().info(f"Evaluating model over {num_episodes} episodes...")
        
        success_count = 0
        total_rewards = []
        episode_lengths = []
        
        for episode in range(num_episodes):
            obs, _ = self.reset()
            episode_reward = 0
            steps = 0
            
            while not self.done and steps < self.episode_length:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, done, info = self.step(action)
                episode_reward += reward
                steps += 1
            
            total_rewards.append(episode_reward)
            episode_lengths.append(steps)
            
            if info["distance_to_goal"] < self.success_threshold:
                success_count += 1
            
            self.get_logger().info(
                f"Episode {episode+1}: Reward={episode_reward:.1f}, "
                f"Steps={steps}, Success={info['distance_to_goal'] < self.success_threshold}"
            )
        
        success_rate = (success_count / num_episodes) * 100
        avg_reward = np.mean(total_rewards)
        avg_steps = np.mean(episode_lengths)
        
        self.get_logger().info(
            f"Evaluation complete: Success rate={success_rate:.1f}%, "
            f"Avg reward={avg_reward:.1f}, Avg steps={avg_steps:.1f}"
        )

def main(args=None):
    rclpy.init(args=args)
    node = MecanumRLController()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()